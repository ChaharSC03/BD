{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"11NJmW9tEQ1n8eAb20aQkQOcd2teLXWQP","authorship_tag":"ABX9TyMetC1Eon/3JHxm0CKXkbnX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**word-count problem**"],"metadata":{"id":"qXAARTmJ6qjZ"}},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23NA8zQ0LZtK","executionInfo":{"status":"ok","timestamp":1686769324641,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sumit Chahar","userId":"13951483046471536732"}},"outputId":"335d1d90-9676-4a7e-8a81-328bc8f0b978"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('Lorem', 2), ('Ipsum', 2), ('is', 1), ('simply', 1), ('dummy', 2), ('text', 2), ('of', 2), ('the', 4), ('printing', 1), ('and', 2), ('typesetting', 1), ('industry.', 1), ('has', 2), ('been', 1), (\"industry's\", 1), ('standard', 1), ('ever', 1), ('since', 1), ('1500s,', 1), ('when', 1), ('an', 1), ('unknown', 1), ('printer', 1), ('took', 1), ('a', 2), ('galley', 1), ('type', 2), ('scrambled', 1), ('it', 1), ('to', 1), ('make', 1), ('specimen', 1), ('book.', 1), ('It', 1), ('survived', 1), ('not', 1), ('only', 1), ('five', 1), ('centuries,', 1), ('but', 1), ('also', 1), ('leap', 1), ('into', 1), ('electronic', 1), ('typesetting,', 1), ('remaining', 1), ('essentially', 1), ('unchanged.', 1)]\n","48\n"]}],"source":["# word-count problem\n","\n","# from collections import Counter\n","\n","def mapper(text):\n","  words=text.split()\n","  keyValue_pair=[(word,1) for word in words]\n","  return keyValue_pair\n","\n","def reducer(key, value):\n","  word_count=sum(value)\n","  return (key,word_count)\n","\n","def mapReducer(data, mapper, reducer):\n","  #map\n","  inter_result=[]\n","  for item in data:\n","    inter_result.extend(mapper(item))\n","\n","\n","  #reducer\n","  grouping={}\n","  for key, value in inter_result:\n","    if key not in grouping:\n","      grouping[key]=[]\n","\n","    grouping[key].append(value)\n","\n","  result=[]\n","  for key, value in grouping.items():\n","    result.append(reducer(key,value))\n","\n","  return result\n","\n","text=[\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.\"]\n","\n","\n","res=mapReducer(text,mapper, reducer)\n","print(res)\n","\n","print(len(res))"]},{"cell_type":"markdown","source":["**bloom** **fiter**"],"metadata":{"id":"lLiNiISn6w3k"}},{"cell_type":"code","source":["# bloom fiter\n","# !pip install mmh3\n","# !pip install bitarray\n","\n","import mmh3\n","from bitarray import bitarray\n","\n","\n","class BF:\n","  def __init__(self, size, num_hash):\n","    self.size=size\n","    self.num_hash=num_hash\n","    self.bit_array=bitarray(size)\n","    self.bit_array.setall(0)\n","\n","  def hashed(self,item):\n","    hashes=[]\n","    for seed in range(self.num_hash):\n","      hashVal=mmh3.hash(item, seed) % self.size\n","      hashes.append(hashVal)\n","    return hashes\n","\n","  def add(self, item):\n","    hashes=self.hashed(item)\n","    for hash in hashes:\n","      self.bit_array[hash]=1\n","\n","  def __contains__ (self, item):\n","    hashes=self.hashed(item)\n","    for hashVal in hashes:\n","      if self.bit_array[hashVal]==0:\n","        return False\n","    return True\n","\n","\n","bloom=BF(10000,3)\n","\n","bloom.add(\"sumit\")\n","bloom.add(\"sumit03\")\n","bloom.add(\"03Sumit\")\n","bloom.add(\"03sumit\")\n","\n","print(\"sumit\" in bloom)\n","print(\"sumit_03\" in bloom)\n","\n","\n","\n"],"metadata":{"id":"pTbB5gY-LpyD","executionInfo":{"status":"ok","timestamp":1686771035842,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sumit Chahar","userId":"13951483046471536732"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b1f9fca6-6f08-4c09-dee2-f66bd75fd709"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}]},{"cell_type":"markdown","source":["**aprior algo**"],"metadata":{"id":"PtRAVGsi60vd"}},{"cell_type":"code","source":["# aprior algo\n","\n","from collections import defaultdict\n","\n","\n","def generate_candidates(freq_itemset, k):\n","  candidates=set()\n","  for itemset1 in freq_itemset:\n","    for itemset2 in freq_itemset:\n","      union_set=itemset1.union(itemset2)\n","      if len(union_set) == k:\n","        candidates.add(union_set)\n","  return candidates\n","\n","def filter_candidates(dataset, candidates, minSupport):\n","  itemset_count=defaultdict(int)\n","  freq_itemset=set()\n","\n","  for transact in dataset:\n","    for candidate in candidates:\n","      if candidate.issubset(transact):\n","        itemset_count[candidate]+=1\n","\n","  num_transact =len(dataset)\n","  for itemset, count in itemset_count.items():\n","    support = count/num_transact\n","    if support >= minSupport:\n","      freq_itemset.add(itemset)\n","\n","  return freq_itemset\n","\n","def apriori(dataset, minSupport):\n","  itemset_count=defaultdict(int)\n","  freq_itemset=set()\n","\n","  # count occurance\n","  for transact in dataset:\n","    for item in transact:\n","      itemset_count[frozenset([item])]+=1\n","\n","  num_transact =len(dataset)\n","  for item, count in itemset_count.items():\n","    support = count/num_transact\n","    if support >= minSupport:\n","      freq_itemset.add(item)\n","\n","  k=2\n","  while freq_itemset:\n","    candidates=generate_candidates(freq_itemset,k)\n","    freq_itemset=filter_candidates(dataset, candidates, minSupport)\n","\n","    for item in freq_itemset:\n","      print(item)\n","\n","    k+=1\n","\n","\n","dataset =[\n","    {'a','b','c'},\n","    {'b','c','d'},\n","    {'a','c','b'},\n","    {'a','c'},\n","    {'a','b','c','d'},\n","    {'b','d'}\n","]\n","\n","minSupport=0.5\n","\n","apriori(dataset,minSupport)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"at9ji_LHTTOG","executionInfo":{"status":"ok","timestamp":1686779878283,"user_tz":-330,"elapsed":633,"user":{"displayName":"Sumit Chahar","userId":"13951483046471536732"}},"outputId":"4c7df3ed-be18-499c-8e06-cb9fefc8650b"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["frozenset({'b', 'c'})\n","frozenset({'b', 'a'})\n","frozenset({'c', 'a'})\n","frozenset({'b', 'd'})\n","frozenset({'b', 'c', 'a'})\n"]}]},{"cell_type":"markdown","source":["**PCY**"],"metadata":{"id":"O6saWmeL6-JZ"}},{"cell_type":"code","source":["import collections\n","import itertools\n","\n","def hash(item1, item2):\n","  \"\"\"Hash function for the hash table.\"\"\"\n","  return (item1 + item2) % 1000\n","\n","def create_bitmap(hash_table, threshold):\n","  \"\"\"Convert the hash table into a bitmap.\"\"\"\n","  bit_map = collections.defaultdict(int)\n","  for key, value in hash_table.items():\n","    bit_map[value] = 1\n","  return bit_map\n","\n","def create_candidate_item_set(dataset):\n","  \"\"\"Create a dictionary of all candidate item sets from the data set with their corresponding count.\"\"\"\n","  candidate_item_list = collections.defaultdict(int)\n","  for transaction in dataset:\n","    for i in range(len(transaction) - 1):\n","      for j in range(i + 1, len(transaction)):\n","        item1 = transaction[i]\n","        item2 = transaction[j]\n","        hash_value = hash(item1, item2)\n","        candidate_item_list[hash_value] += 1\n","  return candidate_item_list\n","\n","def pcy_algorithm(dataset, min_threshold):\n","  \"\"\"Implement the PCY algorithm to find frequent item sets.\"\"\"\n","  # Create a hash table of all pairs of items in the data set.\n","  hash_table = {}\n","  for transaction in dataset:\n","    for i in range(len(transaction) - 1):\n","      for j in range(i + 1, len(transaction)):\n","        item1 = transaction[i]\n","        item2 = transaction[j]\n","        hash_value = hash(item1, item2)\n","        hash_table[hash_value] = 1 if hash_value not in hash_table else hash_table[hash_value] + 1\n","\n","  # Convert the hash table into a bitmap.\n","  bit_map = create_bitmap(hash_table, min_threshold)\n","\n","  # Create a dictionary of all candidate item sets from the data set with their corresponding count.\n","  candidate_item_list = create_candidate_item_set(dataset)\n","\n","  # Find the frequent items from the candidate_item_list.\n","  frequent_item_set = set()\n","  for item, count in candidate_item_list.items():\n","    if count >= min_threshold:\n","      frequent_item_set.add(item)\n","  return frequent_item_set\n","\n","if __name__ == \"__main__\":\n","  # Create some self-made data.\n","  dataset = [[1, 2, 3], [2, 3, 4], [3, 4, 5], [1, 2, 5], [1, 3, 5], [2, 4, 5]]\n","\n","  # Set the minimum support threshold.\n","  min_threshold = 2\n","\n","  # Find the frequent item sets using the PCY algorithm.\n","  frequent_item_set = pcy_algorithm(dataset, min_threshold)\n","\n","  # Print the frequent item sets.\n","  for item in frequent_item_set:\n","    print(item)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KOf72nF36f6S","executionInfo":{"status":"ok","timestamp":1686782389511,"user_tz":-330,"elapsed":705,"user":{"displayName":"Sumit Chahar","userId":"13951483046471536732"}},"outputId":"5446a60d-8cf4-4656-8d7d-2281510e43d1"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","4\n","5\n","6\n","7\n","8\n","9\n"]}]},{"cell_type":"markdown","source":["### HADOOP"],"metadata":{"id":"4hF4FnI_F-ar"}},{"cell_type":"markdown","source":["PROCEDURE â€“\n","Format the Namenode - Formatting the NameNode is done once when hadoop is installed and not for running hadoop filesystem, else it will delete all the data inside HDFS.\n","Run this command-\n","\n","hdfs namenode -format"],"metadata":{"id":"RMQwrNxeF6KP"}},{"cell_type":"markdown","source":["Now let us start with start the dfs and yarn command, for this this cmd files are available in sbin,we have already given path of sbin in system path variables so we dont need to get inside the sbin folder to run the file. This step involves starting the namenode and datanode with this commands:\n","\n","start-dfs.cmd\n","\n","start-yarn.cmd"],"metadata":{"id":"5OPJ2mDcGMIk"}},{"cell_type":"markdown","source":["By perfoming above commands you will get four Apache Distribution windows. Now you can go to this url:http://localhost:9870/ where you can get status of your dfs,namenode and datanode.\n","\n","\n"],"metadata":{"id":"20eDsbQmGcJk"}},{"cell_type":"markdown","source":["Working with HDFS:- Creating a demo file.txt in local file system ,inorder to put it in hdfs usjng hdfs command line tool. Create directory name /user using the following command\n","\n","hdfs dfs -mkdir /user\n","\n","hdfs dfs -ls"],"metadata":{"id":"aCrRa7xLGes8"}},{"cell_type":"markdown","source":["As now you can see new folder /user is created in hdfs. Now,then we will create a txt file in our local file system ,so that we can put it to hdfs using following commands\n","\n","hdfs dfs -copyFromLocal address of txt file /user\n","\n","hdfs dfs -ls/\n","\n","hdfs dfs -ls/user\n"],"metadata":{"id":"BhIGq_xdGudg"}},{"cell_type":"markdown","source":["As,you can see now RLabInst.txt is now store in our hdfs file system. To see it whats inside the file we can perform this command,\n","\n","hdfs dfs -cat data.txt"],"metadata":{"id":"c4TOGY-9HFSG"}},{"cell_type":"markdown","source":["CONCLUSION - We have successfully performed the experiment to implement Hadoop file system."],"metadata":{"id":"yrs228ZoHPwp"}},{"cell_type":"code","source":[],"metadata":{"id":"LC_-bhQsHNjZ"},"execution_count":null,"outputs":[]}]}